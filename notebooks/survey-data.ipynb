{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:17:38.608300Z",
     "start_time": "2025-04-28T18:17:37.575748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Had issues installing pyreadstat so used magic command in notebook instead\n",
    "%pip install pyreadstat"
   ],
   "id": "e4d2f7b2de23c373",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyreadstat in /Users/majaculjak/anaconda3/envs/dsp/lib/python3.11/site-packages (1.2.8)\r\n",
      "Requirement already satisfied: pandas>=1.2.0 in /Users/majaculjak/anaconda3/envs/dsp/lib/python3.11/site-packages (from pyreadstat) (2.2.3)\r\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/majaculjak/anaconda3/envs/dsp/lib/python3.11/site-packages (from pandas>=1.2.0->pyreadstat) (2.0.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/majaculjak/anaconda3/envs/dsp/lib/python3.11/site-packages (from pandas>=1.2.0->pyreadstat) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/majaculjak/anaconda3/envs/dsp/lib/python3.11/site-packages (from pandas>=1.2.0->pyreadstat) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/majaculjak/anaconda3/envs/dsp/lib/python3.11/site-packages (from pandas>=1.2.0->pyreadstat) (2023.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/majaculjak/anaconda3/envs/dsp/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->pyreadstat) (1.17.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 155
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:17:38.613294Z",
     "start_time": "2025-04-28T18:17:38.610043Z"
    }
   },
   "source": [
    "import glob\n",
    "import pyreadstat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "execution_count": 156
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:17:38.616363Z",
     "start_time": "2025-04-28T18:17:38.614187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Establish path to data folder\n",
    "DATA_DIR = Path('/Volumes/WRKGRP/STD-FSW-BSI-SD-Movement_Tracking/dsp/data')\n",
    "\n",
    "#todo make cross-platform safe, or just add your own absolute path to the data folder "
   ],
   "id": "859e858140ec1c4a",
   "outputs": [],
   "execution_count": 157
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tracking data\n",
    "\n",
    "## List of available tracking data\n",
    "\n",
    "Determine available tracking data per date, school, and class by subfolder name.\n",
    "\n",
    "⚠️ Currently only 2023 tracking data has been loaded since we do not have the survey responses to 2022 data.  "
   ],
   "id": "3a760b6a32c759d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:17:38.915565Z",
     "start_time": "2025-04-28T18:17:38.618469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filename = 'tracking-data-all'\n",
    "path = DATA_DIR / '02_interim' / f\"{filename}.csv\"\n",
    "\n",
    "summary_filename = 'tracking-data-summary'\n",
    "summary_path = DATA_DIR / '02_interim' / f\"{summary_filename}.csv\"\n",
    "\n",
    "if path.exists() and summary_path.exists():\n",
    "    print(f'Found {path}')\n",
    "    tracking_data = pd.read_csv(path).astype(str)\n",
    "    tracking_data['date'] = pd.to_datetime(tracking_data['date'])\n",
    "    \n",
    "    print(f'Found {summary_path}')\n",
    "    tracking_summary = pd.read_csv(summary_path).astype(str)\n",
    "    tracking_summary['date'] = pd.to_datetime(tracking_summary['date'])    \n",
    "else:\n",
    "    # Get 2023 school/class info from tracking data subfolder names\n",
    "    folder = DATA_DIR / '01_tracking' / '2023'\n",
    "    subfolders = [f.name for f in folder.iterdir() if f.is_dir()]\n",
    "    \n",
    "    # Initialize list to store all tracking data entries\n",
    "    all_data = []\n",
    "    \n",
    "    # Process each subfolder\n",
    "    for subfolder in subfolders:\n",
    "        date, school_num = subfolder.split('_s')\n",
    "        school_num, class_num = school_num.split('_c')\n",
    "        \n",
    "        # Get the full path to the subfolder\n",
    "        subfolder_path = folder / subfolder\n",
    "        \n",
    "        # Find all CSV files in the subfolder\n",
    "        csv_files = list(subfolder_path.glob('*.csv'))\n",
    "        \n",
    "        if csv_files:\n",
    "            # Process each CSV file in the subfolder\n",
    "            for csv_file in csv_files:\n",
    "                file_name = csv_file.name\n",
    "                \n",
    "                # Extract tracklab_id from filename by splitting on '] ' and taking the part after\n",
    "                if '] ' in file_name:\n",
    "                    tracklab_id = file_name.split('] ', 1)[1]\n",
    "                    # Remove .csv extension if present\n",
    "                    tracklab_id = tracklab_id.replace('.csv', '')\n",
    "                else:\n",
    "                    # Handle case where the expected delimiter isn't found\n",
    "                    tracklab_id = file_name.replace('.csv', '')\n",
    "                \n",
    "                # Add entry for this specific CSV file\n",
    "                all_data.append([date, school_num, class_num, tracklab_id])\n",
    "        else:\n",
    "            # If no CSV files found, still add the folder info without tracklab_id\n",
    "            all_data.append([date, school_num, class_num, None])\n",
    "    \n",
    "    # Create df with date, school, class, and tracklab_id\n",
    "    tracking_data = pd.DataFrame(all_data, columns=['date', 'school', 'class', 'tracklab_id'])\n",
    "    tracking_data = tracking_data.astype(str)\n",
    "    tracking_data['date'] = pd.to_datetime(tracking_data['date'], format='%Y_%m_%d')\n",
    "    \n",
    "    # Create summary df of unique date, school, and class combinations\n",
    "    tracking_summary= tracking_data[['date', 'school', 'class']].drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # Save dfs as csv\n",
    "    summary_filename = 'tracking-data-summary'\n",
    "    summary_path = DATA_DIR / '02_interim' / f\"{summary_filename}.csv\"\n",
    "    summary_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tracking_summary.to_csv(summary_path, index=False)\n",
    "    print(\"Summary data saved to\", summary_path)\n",
    "    \n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tracking_data.to_csv(path, index=False)\n",
    "    print(\"All data saved to\", path)\n",
    "    \n",
    "    display(tracking_data)\n",
    "\n",
    "tracking_data.groupby(['date', 'school', 'class'])['tracklab_id'].nunique()"
   ],
   "id": "1c66ec8fd92936dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found /Volumes/WRKGRP/STD-FSW-BSI-SD-Movement_Tracking/dsp/data/02_interim/tracking-data-all.csv\n",
      "Found /Volumes/WRKGRP/STD-FSW-BSI-SD-Movement_Tracking/dsp/data/02_interim/tracking-data-summary.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "date        school  class\n",
       "2023-04-11  42      102      16\n",
       "2023-05-11  43      103      30\n",
       "2023-05-23  46      107      20\n",
       "2023-05-24  47      108      15\n",
       "2023-05-31  1       104      22\n",
       "2023-06-08  45      105      22\n",
       "2023-06-09  45      106      16\n",
       "Name: tracklab_id, dtype: int64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 158
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load TrackLabID keyfiles",
   "id": "dfbf158ba9a6f435"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:17:39.000253Z",
     "start_time": "2025-04-28T18:17:38.916867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filename = 'keyfile_tracklab_id'\n",
    "path = DATA_DIR / '02_interim' / f\"{filename}.csv\"\n",
    "\n",
    "if path.exists():\n",
    "    # Load formatted keyfile\n",
    "    print(f'Found {path}')\n",
    "    keyfile_tagID = pd.read_csv(path).astype(str)\n",
    "    display(keyfile_tagID)\n",
    "else:\n",
    "    # Load and format raw keyfile + save \n",
    "    path_raw = DATA_DIR / 'keyfiles' / 'Keyfile_csv.csv'\n",
    "    keyfile_tagID = pd.read_csv(path_raw, delimiter=';')\n",
    "    \n",
    "    keyfile_tagID = keyfile_tagID.astype(str)\n",
    "    keyfile_tagID = keyfile_tagID.rename(columns={'Tagnumber': 'tagnumber', 'TrackLabID': 'tracklab_id', 'SubjectID': 'subject_id'})\n",
    "    \n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    keyfile_tagID.to_csv(path, index=False)\n",
    "    print(\"Data saved to\", path)"
   ],
   "id": "ba2b2b1fd9ee4e11",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found /Volumes/WRKGRP/STD-FSW-BSI-SD-Movement_Tracking/dsp/data/02_interim/keyfile_tracklab_id.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   tagnumber     tracklab_id subject_id\n",
       "0          1  0x24025F48A3E6        nan\n",
       "1          2  0x24025F48A133        nan\n",
       "2          3  0x24025F44F8D7        nan\n",
       "3          4  0x24046130B076        nan\n",
       "4          5  0x24046131F437        nan\n",
       "..       ...             ...        ...\n",
       "65       131             nan        nan\n",
       "66       132             nan        nan\n",
       "67       133             nan        nan\n",
       "68       134             nan        nan\n",
       "69       135             nan        nan\n",
       "\n",
       "[70 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tagnumber</th>\n",
       "      <th>tracklab_id</th>\n",
       "      <th>subject_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0x24025F48A3E6</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0x24025F48A133</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0x24025F44F8D7</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0x24046130B076</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0x24046131F437</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>131</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>132</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>133</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>134</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>135</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 159
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Keyfiles",
   "id": "3101912dfad31d10"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:17:39.756982Z",
     "start_time": "2025-04-28T18:17:39.001125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "folder = DATA_DIR / 'keyfiles'\n",
    "# file_path = folder.glob('*.xlsx')\n",
    "\n",
    "keyfiles = pd.DataFrame()\n",
    "\n",
    "for file in folder.glob('*.xlsx'):\n",
    "    try:\n",
    "        df = pd.read_excel(file, engine='openpyxl')\n",
    "        df['source'] = file.stem\n",
    "        keyfiles = pd.concat([keyfiles, df], ignore_index=True)\n",
    "        print(f'Loaded: {file.name}')\n",
    "    except Exception as e:\n",
    "        print(f'Could not read {file.name}: {e}')"
   ],
   "id": "21e3efe198cfc75f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: keyfile school 47 class 108.xlsx\n",
      "Loaded: keyfile school 1 class 104.xlsx\n",
      "Loaded: keyfile school 41 class 100.xlsx\n",
      "Loaded: keyfile school 41 class 101.xlsx\n",
      "Loaded: keyfile school 42 class 102.xlsx\n",
      "Loaded: keyfile school 43 class 103.xlsx\n",
      "Loaded: keyfile school 45 class 105.xlsx\n",
      "Loaded: keyfile school 45 class 106.xlsx\n",
      "Loaded: keyfile school 46 class 107.xlsx\n",
      "Could not read ~$keyfile school 43 class 103.xlsx: File is not a zip file\n"
     ]
    }
   ],
   "execution_count": 160
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:17:39.765665Z",
     "start_time": "2025-04-28T18:17:39.758382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert all entries to string\n",
    "keyfiles = keyfiles.apply(lambda x: x.apply(lambda y: str(int(y)) if pd.notna(y) and isinstance(y, (float, int)) else y))"
   ],
   "id": "a85cda5362e281b",
   "outputs": [],
   "execution_count": 161
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Keyfiles **do not** share the same structure. Columns containing tag numbers are called tagnummer, tagnummer , tagnr, etc. Teachers are not entered according to an ID number but mostly denoted as 'leerkracht'. \n",
    "\n",
    "I'm merging the columns containing tag numbers into one column and assigning school and class numbers to techers, incl. ID number '9999'. "
   ],
   "id": "31931f0e5ed50096"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:17:39.770046Z",
     "start_time": "2025-04-28T18:17:39.766997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a new column 'tagnumber' that combines all the tag number columns\n",
    "keyfiles['tagnumber'] = keyfiles['tagnummer'].copy()\n",
    "\n",
    "# Inspect column names\n",
    "print(keyfiles.columns)"
   ],
   "id": "f62ce020c5b06084",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['school ID', 'klas ID', 'subject ID', 'id', 'voornaam', 'achternaam',\n",
      "       'consent', 'tagnummer', 'source', 'tagnummer ', 'tagnr.', 'Unnamed: 8',\n",
      "       'tagnr', 'trackingnnumer', 'Unnamed: 9', 'Unnamed: 10', 'tagnumber'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 162
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:17:39.775098Z",
     "start_time": "2025-04-28T18:17:39.770923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# List of tag number columns\n",
    "tag_columns = ['tagnummer', 'tagnummer ', 'tagnr.', 'tagnr', 'trackingnnumer']\n",
    "\n",
    "# Fill NaN values in 'tagnumber' with values from other tag columns\n",
    "for col in tag_columns:\n",
    "    if col != 'tagnummer':  # Skip the first column as we already copied it\n",
    "        keyfiles['tagnumber'] = keyfiles['tagnumber'].fillna(keyfiles[col])\n",
    "\n",
    "keyfiles = keyfiles.drop(columns=tag_columns)"
   ],
   "id": "963ab45c1f018d5e",
   "outputs": [],
   "execution_count": 163
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:17:39.789117Z",
     "start_time": "2025-04-28T18:17:39.778215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the list of all columns in the DataFrame\n",
    "all_columns = keyfiles.columns.tolist()\n",
    "\n",
    "# Find the indices of 'source' and 'tagnumber' columns\n",
    "source_index = all_columns.index('source')\n",
    "tagnumber_index = all_columns.index('tagnumber')\n",
    "\n",
    "columns_between = all_columns[source_index+1:tagnumber_index]\n",
    "\n",
    "# Stack their entries in one series\n",
    "fill_values = keyfiles[columns_between].stack().groupby(level=0).first()\n",
    "\n",
    "# Unify them in new column 'comment', then remove individual columns\n",
    "keyfiles['comment'] = np.nan\n",
    "keyfiles['comment'] = keyfiles['comment'].fillna(fill_values)\n",
    "keyfiles = keyfiles.drop(columns = columns_between)"
   ],
   "id": "138baeae9713efe7",
   "outputs": [],
   "execution_count": 164
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Clean tag numbers\n",
    "1. Remove rows where tag numbers are empty or not a number"
   ],
   "id": "f28b34a14bf15a5a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:17:39.792928Z",
     "start_time": "2025-04-28T18:17:39.789986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inspect values in tag number column\n",
    "keyfiles['tagnumber'].unique()"
   ],
   "id": "1161d20b29a143aa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['9', '19', '14', '29', '28', '31', '33', '22', '5', '20', '2',\n",
       "       '30', '32', '18', '11', '1', '3', '34', '10', nan, '27', '25', '-',\n",
       "       '24', '15', '7', '17', '6', '21', '13', '26', '35',\n",
       "       'niet aanwezig, uit vragenlijst gehaald', 'niet aanwezig', '12',\n",
       "       'x '], dtype=object)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 165
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:17:39.799410Z",
     "start_time": "2025-04-28T18:17:39.793744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Keep only rows containing digits in tag number column\n",
    "keyfiles = keyfiles[\n",
    "    keyfiles['tagnumber'].notna() & \n",
    "    keyfiles['tagnumber'].astype(str).str.isdigit()\n",
    "]\n",
    "\n",
    "# Inspect values in tag number again -> OK\n",
    "keyfiles['tagnumber'].unique()"
   ],
   "id": "8404c542ef6aa49c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['9', '19', '14', '29', '28', '31', '33', '22', '5', '20', '2',\n",
       "       '30', '32', '18', '11', '1', '3', '34', '10', '27', '25', '24',\n",
       "       '15', '7', '17', '6', '21', '13', '26', '35', '12'], dtype=object)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 166
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:17:39.811824Z",
     "start_time": "2025-04-28T18:17:39.801382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Rename columns for alignment\n",
    "keyfiles = keyfiles.rename(columns={'school ID': 'school', 'klas ID': 'class'})\n",
    "\n",
    "# Add dates to keyfile by mapping\n",
    "keyfiles['date'] = np.nan\n",
    "date_map = tracking_data.set_index(['school', 'class'])['date'].to_dict()\n",
    "\n",
    "# # Update 'date' in keyfiles where keys match\n",
    "keyfiles['date'] = keyfiles.apply(\n",
    "    lambda row: date_map.get((row['school'], row['class']), row['date']),\n",
    "    axis=1\n",
    ")"
   ],
   "id": "1d593dd15bac10a4",
   "outputs": [],
   "execution_count": 167
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:17:39.907842Z",
     "start_time": "2025-04-28T18:17:39.813895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Rename columns for alignment (fixed typo in variable name)\n",
    "# keyfiles = keyfiles.rename(columns={'tagnummer': 'tagnumber'})\n",
    "# keyfile_tagID = keyfile_tagID.rename(columns={'Tagnumber': 'tagnumber', 'TrackLabID': 'tracklabID'}).astype(str)\n",
    "\n",
    "# Create mapping dictionary - convert keys to same type as keyfiles['tagnumber']\n",
    "tracklab_id_map = keyfile_tagID.dropna(subset=['tagnumber']).set_index('tagnumber')['tracklab_id'].to_dict()\n",
    "\n",
    "# Map values more efficiently using map() instead of apply\n",
    "keyfiles['tracklab_id'] = keyfiles['tagnumber'].map(tracklab_id_map)"
   ],
   "id": "282ba5307e605f70",
   "outputs": [],
   "execution_count": 168
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:17:39.935013Z",
     "start_time": "2025-04-28T18:17:39.909791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "keyfiles = keyfiles.rename(columns={'subject ID': 'subject_id'})\n",
    "\n",
    "# Inspect consent entries\n",
    "print('Entries in CONSENT:')\n",
    "print(keyfiles['consent'].unique())\n",
    "\n",
    "# Get position of 'source' column\n",
    "source_position = list(keyfiles.columns).index('source')\n",
    "\n",
    "# Identify columns before 'source'\n",
    "cols_before_source = keyfiles.columns[:source_position]\n",
    "\n",
    "# Check if any of these columns contain 'leerkracht'\n",
    "has_leerkracht = keyfiles[cols_before_source].apply(\n",
    "    lambda col: col.astype(str).str.contains('leerkracht', case=False, na=False)\n",
    ").any(axis=1)\n",
    "\n",
    "# Assign '9999' to 'subject_ID' where 'leerkracht' was found\n",
    "keyfiles.loc[has_leerkracht, 'subject_id'] = '9999'\n",
    "\n",
    "# Replace positive non-digit consent entries with '1'\n",
    "keyfiles['consent'] = keyfiles['consent'].replace(\n",
    "    {'ja': '1', 'leerkracht': '1'}\n",
    ")\n",
    "\n",
    "# Inspect consent entries\n",
    "print('Entries in CONSENT:')\n",
    "print(keyfiles['consent'].unique())"
   ],
   "id": "f8d67e6d3ea434d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries in CONSENT:\n",
      "['1' nan 'ja' 'leerkracht'\n",
      " 'received oral permission from parent on day of data collection']\n",
      "Entries in CONSENT:\n",
      "['1' nan 'received oral permission from parent on day of data collection']\n"
     ]
    }
   ],
   "execution_count": 169
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:17:39.950796Z",
     "start_time": "2025-04-28T18:17:39.942533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#todo find out why school 44 has been (inconsistently) renamed school 1, and how we should call it\n",
    "keyfiles['school'] = keyfiles['school'].astype(str)\n",
    "# keyfiles.loc[keyfiles['school']=='44', 'school'] = '1'"
   ],
   "id": "6013be43fdadca0",
   "outputs": [],
   "execution_count": 170
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Teacher responses\n",
    "\n",
    "Teacher survey responses look like they were collected through an online form. Delivered raw as wide-format SPSS files. \n",
    "\n",
    "# IOP scores\n",
    "\n",
    "Confirmed with Nathalie that IOP responses were optional.\n",
    "If IOP response was given, variable name contains 'Q68' and student number. It's then followed by Q70 and Q71 with matching student number.\n",
    "\n",
    "School and class data has been added to each dataframe from source filename. This data is surely already present in the questionnaire, but I cannot decipher under which variable it's present. Hence, a workaround."
   ],
   "id": "2970f2f24aa8e85d"
  },
  {
   "cell_type": "code",
   "id": "4ab5c31f7f61c73f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:18:00.621433Z",
     "start_time": "2025-04-28T18:17:39.952910Z"
    }
   },
   "source": [
    "# Determine path to raw SPSS files\n",
    "folder = DATA_DIR / '01_survey' / 'teacher_raw_2023'\n",
    "\n",
    "# Initiate empty dict to store teacher questionnaire dfs\n",
    "tq_all = {}\n",
    "\n",
    "for file in folder.glob('*.sav'):\n",
    "    var_name = file.stem\n",
    "    df = pd.read_spss(file)\n",
    "    \n",
    "    # Add school/class as columns to each df from filename\n",
    "    school_num, class_num = var_name.split('_')[1:]\n",
    "    df['school'] = school_num\n",
    "    df['class'] = class_num\n",
    "    \n",
    "    # Store df in dict with filename as key\n",
    "    tq_all[var_name] = df\n",
    "    print(f'Loaded dataframe: {var_name}')\n",
    "    \n",
    "print(f'Total dataframes: {len(tq_all)}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataframe: tq_49_113\n",
      "Loaded dataframe: tq_1_104\n",
      "Loaded dataframe: tq_41_100\n",
      "Loaded dataframe: tq_41_101\n",
      "Loaded dataframe: tq_42_102\n",
      "Loaded dataframe: tq_43_103\n",
      "Loaded dataframe: tq_45_105\n",
      "Loaded dataframe: tq_45_106\n",
      "Loaded dataframe: tq_46_107\n",
      "Loaded dataframe: tq_47_108\n",
      "Loaded dataframe: tq_49_110\n",
      "Loaded dataframe: tq_49_111\n",
      "Loaded dataframe: tq_49_112\n",
      "Total dataframes: 13\n"
     ]
    }
   ],
   "execution_count": 171
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:18:00.624700Z",
     "start_time": "2025-04-28T18:18:00.622502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Uncomment to inspect example of available columns\n",
    "# print(tq_all['tq_1_104'].columns.tolist())"
   ],
   "id": "6e16680ba75f7215",
   "outputs": [],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:18:00.923169Z",
     "start_time": "2025-04-28T18:18:00.625690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initiate empty dict to store relevant tq only\n",
    "tq_relevant = {}\n",
    "\n",
    "for df in tq_all:\n",
    "    school_num = str(tq_all[df]['school'].iloc[0])\n",
    "    class_num = str(tq_all[df]['class'].iloc[0])\n",
    "    \n",
    "    tq_match = tracking_data[\n",
    "        (tracking_data['school'] == school_num) &\n",
    "        (tracking_data['class'] == class_num)\n",
    "    ]\n",
    "    \n",
    "    if not tq_match.empty:\n",
    "        tq_relevant[df] = tq_all[df]\n",
    "        print(f\"Matching dataframe: {df}\")\n",
    "\n",
    "#todo concatenate AFTER the columns have been equalized\n",
    "# tq = pd.concat(tq_relevant, ignore_index=True)\n",
    "\n",
    "print(f\"Total matching: {len(tq_relevant)}\")"
   ],
   "id": "5036e2a2-5a5d-4e05-ab77-4a920b51d0ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching dataframe: tq_1_104\n",
      "Matching dataframe: tq_42_102\n",
      "Matching dataframe: tq_43_103\n",
      "Matching dataframe: tq_45_105\n",
      "Matching dataframe: tq_45_106\n",
      "Matching dataframe: tq_46_107\n",
      "Matching dataframe: tq_47_108\n",
      "Total matching: 7\n"
     ]
    }
   ],
   "execution_count": 173
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Based on reading the codebook and inspecting the answers in the raw files, I've determined the following:\n",
    "* Q30 = school\n",
    "* Q31 = class\n",
    "* Q32 = ?\n",
    "* Q27 = T_gender\n",
    "* Q28 = T_age\n",
    "* Q29 = T_dutch\n",
    "* Q30.0 = T_exp1\n",
    "* Q31.0 = T_exp2\n",
    "* Q32.0 = T_time_teaching\n",
    "* Q33 = T_class_comp"
   ],
   "id": "183e5b0052c27f11"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:18:00.925887Z",
     "start_time": "2025-04-28T18:18:00.924176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Uncomment to inspect dataset\n",
    "# tq_relevant['tq_1_104']"
   ],
   "id": "5a867e46efa2b343",
   "outputs": [],
   "execution_count": 174
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Q68: \"In vergelijking met andere leerlingen bezoek ik [naam kind]\"\n",
    "\n",
    "Q68 responses: Minder vaak, Gemiddeld, Vaker\n",
    "\n",
    "IOP response Q68 is given per student. Variable name format is 'Q68_N', where N should match an entry in keyfiles['subject ID']. By matching the subject ID, Q68 can then be matched to the 4-digit 'ID' in the file containing student survey responses (once these have been fixed). For an initial analysis, the matching to the 'subject ID' and thus to tracking tag numbers should be enough. "
   ],
   "id": "99952d9c85525ef3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:18:01.016860Z",
     "start_time": "2025-04-28T18:18:00.926650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Eliminating irrelevant columns in tq dataframes\n",
    "\n",
    "# Lists of relevant questions\n",
    "descriptives = ['Q27', 'Q28', 'Q29', 'Q30', 'Q31', 'Q32', 'Q33']\n",
    "iop_id = ['Q68']  # Add 'Q70', 'Q71' for detailed IOP responses\n",
    "\n",
    "tq_filtered = {}\n",
    "\n",
    "for key, df in tq_relevant.items():\n",
    "    # Create a mask for columns to keep\n",
    "    cols_to_keep = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Check if column matches any descriptive column\n",
    "        if any(q_id in col for q_id in descriptives):\n",
    "            cols_to_keep.append(col)\n",
    "        # Check if column contains any of the specified question IDs\n",
    "        elif any(q_id in col for q_id in iop_id):\n",
    "            cols_to_keep.append(col)\n",
    "    \n",
    "    # Create a new dataframe with only the columns to keep\n",
    "    tq_filtered[key] = df[cols_to_keep]"
   ],
   "id": "1dc976f988fae873",
   "outputs": [],
   "execution_count": 175
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load IOP Q68 values into keyfiles df",
   "id": "9d29b40f3a7c7719"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:18:01.542315Z",
     "start_time": "2025-04-28T18:18:01.017681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a new column 'iop' in keyfiles if it doesn't exist\n",
    "if 'iop' not in keyfiles.columns:\n",
    "    keyfiles['iop'] = None  # Initialize with None values\n",
    "\n",
    "# Iterate through each dataframe in the dictionary\n",
    "for df_name, df in tq_filtered.items():\n",
    "    # Make a copy of the dataframe to ensure we're working with a clean copy\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Convert columns Q30 and Q31 to string type using .loc to avoid the warning\n",
    "    df_copy.loc[:, 'Q30'] = df_copy['Q30'].astype(str)\n",
    "    df_copy.loc[:, 'Q31'] = df_copy['Q31'].astype(str)\n",
    "    \n",
    "    # Identify the Q68_N columns (those that start with 'Q68_')\n",
    "    q68_cols = [col for col in df_copy.columns if col.startswith('Q68_')]\n",
    "    \n",
    "    # Iterate through each row in the dataframe\n",
    "    for idx, row in df_copy.iterrows():\n",
    "        school = row['Q30']\n",
    "        class_val = row['Q31']\n",
    "        \n",
    "        # Check each Q68_N column for matching subjects\n",
    "        for q68_col in q68_cols:\n",
    "            # Extract just the number part from Q68_N column name\n",
    "            subject_id = q68_col.split('_')[1]  # Extract the N from Q68_N\n",
    "            \n",
    "            # Get the value from this Q68_N cell\n",
    "            q68_value = row[q68_col]\n",
    "            \n",
    "            # Only proceed if the cell has a valid value\n",
    "            if pd.notna(q68_value) and str(q68_value) != \"0\" and str(q68_value) != \"\":\n",
    "                # Find matching rows in keyfiles where all three conditions are met\n",
    "                matching_rows = keyfiles[(keyfiles['school'] == school) & \n",
    "                                        (keyfiles['class'] == class_val) & \n",
    "                                        (keyfiles['subject_id'] == subject_id)]\n",
    "                \n",
    "                # If matches found, update the 'iop' column with the actual value from Q68_N\n",
    "                if not matching_rows.empty:\n",
    "                    keyfiles.loc[matching_rows.index, 'iop'] = q68_value"
   ],
   "id": "dca09374e1bbb48a",
   "outputs": [],
   "execution_count": 176
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# TEMP: Export merged file\n",
    "\n",
    "File includes connection school & class -> subject_id -> tagnumber -> tracklab_id + iop"
   ],
   "id": "a1ce3dd738091b38"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:22:48.190827Z",
     "start_time": "2025-04-28T18:22:48.179985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "temp = tracking_data.copy().astype(str)\n",
    "temp.loc[temp['school']=='1', 'school'] = '44'"
   ],
   "id": "89afbf7c6a44d510",
   "outputs": [],
   "execution_count": 186
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:22:53.521266Z",
     "start_time": "2025-04-28T18:22:53.510871Z"
    }
   },
   "cell_type": "code",
   "source": "temp[temp['school']=='44']",
   "id": "a5a57af10680d224",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           date school class     tracklab_id\n",
       "101  2023-05-31     44   104  0x24025F44B7A5\n",
       "102  2023-05-31     44   104  0x24025F44DBFA\n",
       "103  2023-05-31     44   104  0x24025F44E6FB\n",
       "104  2023-05-31     44   104  0x24025F44ECCF\n",
       "105  2023-05-31     44   104  0x24025F44F682\n",
       "106  2023-05-31     44   104  0x24025F465724\n",
       "107  2023-05-31     44   104  0x240461308FB5\n",
       "108  2023-05-31     44   104  0x24046130B6FA\n",
       "109  2023-05-31     44   104  0x24046130B9B6\n",
       "110  2023-05-31     44   104  0x24046130BA41\n",
       "111  2023-05-31     44   104  0x24046130BB1E\n",
       "112  2023-05-31     44   104  0x24046130BDD0\n",
       "113  2023-05-31     44   104  0x24046130C8AB\n",
       "114  2023-05-31     44   104  0x24046130C90F\n",
       "115  2023-05-31     44   104  0x24046130CCF9\n",
       "116  2023-05-31     44   104  0x24046130C9C5\n",
       "117  2023-05-31     44   104  0x24046131EE8C\n",
       "118  2023-05-31     44   104  0x24046131EF0E\n",
       "119  2023-05-31     44   104  0x24046131F062\n",
       "120  2023-05-31     44   104  0x24046131F437\n",
       "121  2023-05-31     44   104  0x24046131F584\n",
       "122  2023-05-31     44   104           Tag 1\n",
       "123  2023-05-31     44   104  0x24025F44DBFA\n",
       "124  2023-05-31     44   104  0x24025F44E6FB\n",
       "125  2023-05-31     44   104  0x24025F44ECCF\n",
       "126  2023-05-31     44   104  0x24025F44F682\n",
       "127  2023-05-31     44   104  0x240461308FB5\n",
       "128  2023-05-31     44   104  0x24046130BA41\n",
       "129  2023-05-31     44   104  0x24046130BB1E\n",
       "130  2023-05-31     44   104  0x24046130C9C5\n",
       "131  2023-05-31     44   104  0x24046131EE8C\n",
       "132  2023-05-31     44   104  0x24046131F062\n",
       "133  2023-05-31     44   104  0x24046131F437\n",
       "134  2023-05-31     44   104  0x24046131F584\n",
       "135  2023-05-31     44   104           Tag 1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>school</th>\n",
       "      <th>class</th>\n",
       "      <th>tracklab_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24025F44B7A5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24025F44DBFA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24025F44E6FB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24025F44ECCF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24025F44F682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24025F465724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x240461308FB5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24046130B6FA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24046130B9B6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24046130BA41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24046130BB1E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24046130BDD0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24046130C8AB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24046130C90F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24046130CCF9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24046130C9C5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24046131EE8C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24046131EF0E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24046131F062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24046131F437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24046131F584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>Tag 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24025F44DBFA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24025F44E6FB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24025F44ECCF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24025F44F682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x240461308FB5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24046130BA41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24046130BB1E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24046130C9C5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24046131EE8C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24046131F062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24046131F437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>0x24046131F584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>44</td>\n",
       "      <td>104</td>\n",
       "      <td>Tag 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 188
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:23:03.823220Z",
     "start_time": "2025-04-28T18:23:03.795722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First, let's find the missing tracklab_ids for each school-class combination\n",
    "missing_entries = []\n",
    "\n",
    "# Get unique school-class combinations from keyfiles\n",
    "school_class_combinations = keyfiles[['school', 'class']].drop_duplicates()\n",
    "\n",
    "# For each school-class combination\n",
    "for _, row in school_class_combinations.iterrows():\n",
    "    school = row['school']\n",
    "    class_val = row['class']\n",
    "    \n",
    "    # Get all tracklab_ids for this school-class in tracking_data\n",
    "    tracking_ids = temp[(temp['school'] == school) & \n",
    "                        (temp['class'] == class_val)]['tracklab_id'].unique()\n",
    "    \n",
    "    # Get all tracklab_ids for this school-class already in keyfiles\n",
    "    keyfiles_ids = keyfiles[(keyfiles['school'] == school) & \n",
    "                           (keyfiles['class'] == class_val)]['tracklab_id'].unique()\n",
    "    \n",
    "    # Find tracklab_ids in tracking_data but not in keyfiles\n",
    "    missing_ids = set(tracking_ids) - set(keyfiles_ids)\n",
    "    \n",
    "    # Create new rows for each missing tracklab_id\n",
    "    for missing_id in missing_ids:\n",
    "        # Create a new row with school, class, and tracklab_id\n",
    "        new_row = {\n",
    "            'school': school,\n",
    "            'class': class_val,\n",
    "            'tracklab_id': missing_id\n",
    "        }\n",
    "        missing_entries.append(new_row)\n",
    "\n",
    "# Create DataFrame from the missing entries\n",
    "if missing_entries:\n",
    "    missing_df = pd.DataFrame(missing_entries)\n",
    "    \n",
    "    # Append the missing entries to keyfiles\n",
    "    keyfiles = pd.concat([keyfiles, missing_df], ignore_index=True)\n",
    "    \n",
    "    print(f\"Added {len(missing_entries)} new rows to keyfiles for missing tracklab_ids\")\n",
    "else:\n",
    "    print(\"No missing tracklab_ids found\")\n",
    "\n",
    "# Display the updated keyfiles DataFrame\n",
    "keyfiles.sort_values(by=['school', 'class', 'tracklab_id'])[['school', 'class', 'tracklab_id']]"
   ],
   "id": "9a9f3b3e7834e21c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 10 new rows to keyfiles for missing tracklab_ids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "    school class     tracklab_id\n",
       "37      41   100  0x24025F449A89\n",
       "36      41   100  0x24025F44AD21\n",
       "43      41   100  0x24025F44DBFA\n",
       "47      41   100  0x24025F44E6FB\n",
       "48      41   100  0x24025F44ECCF\n",
       "..     ...   ...             ...\n",
       "178     47   108           Tag 3\n",
       "74     nan   NaN  0x24046130BA41\n",
       "139    nan   NaN  0x24046130BA41\n",
       "155    nan   NaN  0x24046130BA41\n",
       "19     nan   NaN  0x24046131EE8C\n",
       "\n",
       "[212 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>class</th>\n",
       "      <th>tracklab_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>41</td>\n",
       "      <td>100</td>\n",
       "      <td>0x24025F449A89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>41</td>\n",
       "      <td>100</td>\n",
       "      <td>0x24025F44AD21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>41</td>\n",
       "      <td>100</td>\n",
       "      <td>0x24025F44DBFA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>41</td>\n",
       "      <td>100</td>\n",
       "      <td>0x24025F44E6FB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>41</td>\n",
       "      <td>100</td>\n",
       "      <td>0x24025F44ECCF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>47</td>\n",
       "      <td>108</td>\n",
       "      <td>Tag 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x24046130BA41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x24046130BA41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x24046130BA41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x24046131EE8C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 189
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:23:24.819557Z",
     "start_time": "2025-04-28T18:23:24.433733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# temp = tracking_data.copy().astype(str)\n",
    "# temp.loc[temp['school']=='1'] = '44'\n",
    "\n",
    "export = keyfiles.loc[keyfiles['school'].isin(temp['school'].unique())]\n",
    "\n",
    "filename = 'merged-data-WIP'\n",
    "today = pd.to_datetime('today').strftime('%Y-%m-%d_%H-%M')\n",
    "savepath = DATA_DIR / '02_interim' / f\"{filename}_{today}.xlsx\"\n",
    "export.to_excel(savepath, index=False, engine='openpyxl')"
   ],
   "id": "126ff69bd55519a0",
   "outputs": [],
   "execution_count": 190
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Student responses",
   "id": "f058e4332cbd03c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T12:31:27.832089Z",
     "start_time": "2025-04-28T12:31:26.645578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filename = 'TotalData_T1_all_cbs_ethnicity_gender'\n",
    "path = DATA_DIR / '01_survey' / f\"{filename}.xlsx\"\n",
    "\n",
    "students_raw = pd.read_excel(path)"
   ],
   "id": "81136b99db5add28",
   "outputs": [],
   "execution_count": 177
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T12:34:25.551528Z",
     "start_time": "2025-04-28T12:34:25.545177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "students = students_raw.copy()\n",
    "\n",
    "students = students.rename(columns={'School_ID': 'school', 'Class_ID': 'class'})"
   ],
   "id": "4c71a62cd06d4c03",
   "outputs": [],
   "execution_count": 185
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Turn all columns before 'age' to string.\n",
    "\n",
    "This is totally arbitrary typecasting; I just need select columns in this range to be string. "
   ],
   "id": "cf498ac211f9bae0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T12:39:25.926044Z",
     "start_time": "2025-04-28T12:39:25.915830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get position of 'age' column\n",
    "age_position = list(students.columns).index('age')\n",
    "\n",
    "# Identify columns before 'age'\n",
    "cols_before_age = students.columns[:age_position]\n",
    "\n",
    "# Typecast to string\n",
    "students[cols_before_age] = students[cols_before_age].astype('str')"
   ],
   "id": "8189b2a6720021e0",
   "outputs": [],
   "execution_count": 191
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T12:40:29.388166Z",
     "start_time": "2025-04-28T12:40:29.368889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a set of valid (school, class) pairs from tracking_data\n",
    "valid_pairs = set(zip(tracking_data['school'], tracking_data['class']))\n",
    "\n",
    "# Filter students DataFrame\n",
    "students = students[\n",
    "    students.apply(lambda row: (row['school'], row['class']) in valid_pairs, axis=1)\n",
    "]"
   ],
   "id": "76338f7e1f22828c",
   "outputs": [],
   "execution_count": 193
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T12:40:30.024076Z",
     "start_time": "2025-04-28T12:40:30.019335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(students_raw.shape)\n",
    "print(students.shape)"
   ],
   "id": "34ad1db4dfdc525c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(310, 534)\n",
      "(146, 534)\n"
     ]
    }
   ],
   "execution_count": 194
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T12:43:52.247135Z",
     "start_time": "2025-04-28T12:43:52.216875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create list of SPARTS score columns\n",
    "st_rel = [col for col in students_raw.columns if col.startswith('st_rel')]\n",
    "\n",
    "# Get index of 'gender_sr'\n",
    "gender_sr_idx = students.columns.get_loc('gender_sr')\n",
    "\n",
    "# Get all columns up to and including 'gender_sr'\n",
    "base_cols = students.columns[:gender_sr_idx+1]\n",
    "\n",
    "# Combine with st_rel columns\n",
    "students = students[list(base_cols) + st_rel]"
   ],
   "id": "cbfca7c79008efd2",
   "outputs": [],
   "execution_count": 196
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T15:05:42.121471Z",
     "start_time": "2025-04-28T15:05:42.099305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "students_short = students[['school','class','tracking','nPupils','dataPresent','ID','consent','sID','informed','assent']]\n",
    "# students_short"
   ],
   "id": "7b3932c340cfc60a",
   "outputs": [],
   "execution_count": 241
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## SPARTS scores\n",
    "\n",
    "Source:  https://doi.org/10.1111/bjep.12094\n",
    "\n",
    "Relevant variables named 'SPARTSN' (e.g. 'SPARTS1') in the codebook, but this name is not present in the data. Instead, variables named **'st_relN'** have been identified as SPARTS scores. As explained in the codebook, the questionnaire contained 13 items, but Q13 was not presented to all students. After filtering the dataset for relevant data only (i.e., responses of students whose tracking data we have available), only responses 1-12 were available anyway.  \n",
    "\n",
    "Q12 is not part of the original scale, but developed for this study.\n",
    "\n",
    "I cannot find a score sheet for this test that is not behind a paywall. The COTAN entry for the SPARTS lists a 25-item test instead of the 13-item test used. \n",
    "\n",
    "#todo ask Yvonne & Nathalie for scoring sheet "
   ],
   "id": "8c3e5183a77bfaed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
